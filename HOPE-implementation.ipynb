{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3142a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8003c3",
   "metadata": {},
   "source": [
    "The Core Update Rule (Eq. 28 & 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3375140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hope_update_rule(W, x, grad_loss, eta):\n",
    "    \"\"\"\n",
    "    Implements the HOPE optimizer update rule (Eq 28/29 in the paper).\n",
    "    \n",
    "    W_{t+1} = W_t(I - x_t x_t^T) - eta * grad_L\n",
    "    \n",
    "    Args:\n",
    "        W: Current weight state (Batch, Out_Dim, In_Dim)\n",
    "        x: Input vector (Batch, In_Dim, 1)\n",
    "        grad_loss: Gradient of the loss w.r.t W (or simplified: error * x.T)\n",
    "        eta: Learning rate\n",
    "    \"\"\"\n",
    "    # Term 1: W_t (I - x x^T) -> Projection / Forgetting term\n",
    "    # x @ x.transpose: (B, In, 1) @ (B, 1, In) -> (B, In, In)\n",
    "    # I - xxT: (B, In, In)\n",
    "    I = torch.eye(W.shape[-1], device=W.device).unsqueeze(0).expand(W.shape[0], -1, -1)\n",
    "    \n",
    "    # Paper suggests x should be normalized for stability in this update\n",
    "    x_norm = x / (torch.norm(x, dim=1, keepdim=True) + 1e-8)\n",
    "    projection_matrix = I - torch.bmm(x_norm, x_norm.transpose(1, 2))\n",
    "    \n",
    "    term_1 = torch.bmm(W, projection_matrix)\n",
    "    \n",
    "    # Term 2: Gradient update\n",
    "    term_2 = eta * grad_loss\n",
    "    \n",
    "    W_new = term_1 - term_2\n",
    "    return W_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1f4b0",
   "metadata": {},
   "source": [
    " Self-Referential Memory (Attention Replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e49d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOPERecurrentMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    The 'Working Memory' of HOPE. \n",
    "    It learns to compress context (Keys -> Values) into a weight matrix M_t\n",
    "    using the nested update rule, then answers Queries.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, head_dim, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.head_dim = head_dim\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Projections (standard like Transformer)\n",
    "        self.W_q = nn.Linear(dim, head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(dim, head_dim, bias=False)\n",
    "        self.W_v = nn.Linear(dim, head_dim, bias=False)\n",
    "        self.W_o = nn.Linear(head_dim, dim, bias=False)\n",
    "        \n",
    "        # Norms for stability in recurrent updates\n",
    "        self.ln_k = nn.LayerNorm(head_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.W_q(x) # (B, T, H)\n",
    "        k = self.ln_k(self.W_k(x)) # (B, T, H)\n",
    "        v = self.W_v(x) # (B, T, H)\n",
    "        \n",
    "        # Initialize Memory State M_0 (Batch, Head, Head)\n",
    "        # M maps Keys (Head) -> Values (Head)\n",
    "        M = torch.zeros(batch_size, self.head_dim, self.head_dim, device=x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # Sequential processing (Recurrent view of Linear Attention)\n",
    "        # Note: In production, this can be parallelized via chunking/CUDA (like Titans/Mamba)\n",
    "        for t in range(seq_len):\n",
    "            k_t = k[:, t, :].unsqueeze(2) # (B, H, 1)\n",
    "            v_t = v[:, t, :].unsqueeze(2) # (B, H, 1)\n",
    "            q_t = q[:, t, :].unsqueeze(2) # (B, H, 1)\n",
    "            \n",
    "            # 1. Prediction/Loss Calculation (Implicit)\n",
    "            # We want M_t to map k_t -> v_t. \n",
    "            # Error signal: (M_t * k_t - v_t)\n",
    "            pred_v = torch.bmm(M, k_t)\n",
    "            error = pred_v - v_t\n",
    "            \n",
    "            # Gradient of MSE = 2 * error * k_t.T\n",
    "            grad_loss = torch.bmm(error, k_t.transpose(1, 2))\n",
    "            \n",
    "            # 2. Update Memory M_t -> M_{t+1} using HOPE Rule (Eq 29)\n",
    "            M = hope_update_rule(M, k_t, grad_loss, self.lr)\n",
    "            \n",
    "            # 3. Compute Output using Updated Memory\n",
    "            # y_t = M_{t+1} * q_t\n",
    "            y_t = torch.bmm(M, q_t).squeeze(2)\n",
    "            outputs.append(y_t)\n",
    "            \n",
    "        y = torch.stack(outputs, dim=1) # (B, T, H)\n",
    "        return self.W_o(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bad84",
   "metadata": {},
   "source": [
    "Continuum Memory System (CMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc28eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMSLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of the Continuum Memory System.\n",
    "    Concept: It's an MLP that holds state and updates its own weights \n",
    "    based on a local surprise signal at a specific frequency.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim, update_freq=1, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.update_freq = update_freq\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # We hold weights as buffers so we can manually update them per-batch if needed,\n",
    "        # but for this demo, we make them Parameters that update via a custom logic.\n",
    "        # To simulate 'fast weights' per sample is expensive, so we share weights \n",
    "        # across the batch but update them temporally during the forward pass.\n",
    "        self.w1 = nn.Parameter(torch.randn(hidden_dim, dim) * 0.02)\n",
    "        self.w2 = nn.Parameter(torch.randn(dim, hidden_dim) * 0.02)\n",
    "        \n",
    "        self.act = nn.SiLU()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward_computation(self, x, w1, w2):\n",
    "        # x: (B, D)\n",
    "        h = F.linear(x, w1)\n",
    "        h = self.act(h)\n",
    "        return F.linear(h, w2)\n",
    "\n",
    "    def forward(self, x, global_step=0):\n",
    "        \"\"\"\n",
    "        x: (Batch, Seq, Dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        outputs = []\n",
    "        \n",
    "        # Clone current weights for temporary modification (Plasticity)\n",
    "        # In a full implementation, these would be stateful per batch.\n",
    "        curr_w1 = self.w1.clone()\n",
    "        curr_w2 = self.w2.clone()\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            input_t = x[:, t, :]\n",
    "            \n",
    "            # Standard FFN pass\n",
    "            out_t = self.forward_computation(input_t, curr_w1, curr_w2)\n",
    "            outputs.append(out_t)\n",
    "            \n",
    "            # CMS Update Logic (Eq 31)\n",
    "            # Check frequency\n",
    "            current_time = global_step + t\n",
    "            if current_time % self.update_freq == 0:\n",
    "                # Calculate \"Surprise\" / Gradient\n",
    "                # Here we use a self-supervised reconstruction proxy: \n",
    "                # Ideally, CMS tries to predict input_t or reconstruct features.\n",
    "                # For this demo, we treat the FFN output as a prediction of the input (Autoencoder-like)\n",
    "                # to generate a gradient signal.\n",
    "                \n",
    "                loss = F.mse_loss(out_t, input_t) # Simple local objective\n",
    "                \n",
    "                # Manual Gradient (approximate for demo speed)\n",
    "                grad_w1 = torch.autograd.grad(loss, curr_w1, retain_graph=True)[0]\n",
    "                grad_w2 = torch.autograd.grad(loss, curr_w2, retain_graph=True)[0]\n",
    "                \n",
    "                # Apply update (SGD)\n",
    "                curr_w1 = curr_w1 - self.lr * grad_w1\n",
    "                curr_w2 = curr_w2 - self.lr * grad_w2\n",
    "                \n",
    "                # Detach to prevent VRAM explosion (Truncated BPTT equivalent)\n",
    "                curr_w1 = curr_w1.detach().requires_grad_(True)\n",
    "                curr_w2 = curr_w2.detach().requires_grad_(True)\n",
    "\n",
    "        return torch.stack(outputs, dim=1) + x # Residual connection\n",
    "\n",
    "class ContinuumMemorySystem(nn.Module):\n",
    "    \"\"\"\n",
    "    Eq 30: Nested MLPs with different frequencies.\n",
    "    Input -> Fast MLP -> Medium MLP -> Slow MLP -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, expansion_factor=4):\n",
    "        super().__init__()\n",
    "        hidden = dim * expansion_factor\n",
    "        \n",
    "        # Level 1: Fast (Updates every step)\n",
    "        self.fast_mlp = CMSLayer(dim, hidden, update_freq=1, learning_rate=0.1)\n",
    "        \n",
    "        # Level 2: Slow (Updates every 16 steps)\n",
    "        # Representing \"Long-term\" knowledge storage\n",
    "        self.slow_mlp = CMSLayer(dim, hidden, update_freq=16, learning_rate=0.01)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Chain of thoughts / computations\n",
    "        h = self.fast_mlp(x)\n",
    "        out = self.slow_mlp(h)\n",
    "        return self.norm(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf32b8d",
   "metadata": {},
   "source": [
    "Full HOPE Block and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50df6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOPEBlock(nn.Module):\n",
    "    def __init__(self, dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.memory = HOPERecurrentMemory(dim, head_dim)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.cms = ContinuumMemorySystem(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Working Memory (Attention-like)\n",
    "        h = x + self.memory(self.norm1(x))\n",
    "        \n",
    "        # 2. Continuum Memory (FFN-like)\n",
    "        out = h + self.cms(self.norm2(h))\n",
    "        return out\n",
    "\n",
    "class HOPEModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, depth, head_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            HOPEBlock(dim, head_dim) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d713d",
   "metadata": {},
   "source": [
    "Toy Experiment: Associative Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d17386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HOPE Toy Task...\n",
      "Step 1, Loss: 4.7313\n",
      "Step 2, Loss: 4.5962\n",
      "Step 3, Loss: 4.4845\n",
      "Step 4, Loss: 4.3810\n",
      "Step 5, Loss: 4.2878\n",
      "Step 6, Loss: 4.2005\n",
      "Step 7, Loss: 4.1187\n",
      "Step 8, Loss: 4.0420\n",
      "Step 9, Loss: 3.9680\n",
      "Step 10, Loss: 3.9103\n",
      "Step 11, Loss: 3.8326\n",
      "Step 12, Loss: 3.7665\n",
      "Step 13, Loss: 3.7046\n",
      "Step 14, Loss: 3.6426\n",
      "Step 15, Loss: 3.5780\n",
      "Step 16, Loss: 3.5302\n",
      "Step 17, Loss: 3.4673\n",
      "Step 18, Loss: 3.4019\n",
      "Step 19, Loss: 3.3474\n",
      "Step 20, Loss: 3.2902\n",
      "Step 21, Loss: 3.2318\n",
      "Step 22, Loss: 3.1934\n",
      "Step 23, Loss: 3.1251\n",
      "Step 24, Loss: 3.0862\n",
      "Step 25, Loss: 3.0286\n",
      "Step 26, Loss: 2.9789\n",
      "Step 27, Loss: 2.9412\n",
      "Step 28, Loss: 2.9026\n",
      "Step 29, Loss: 2.8404\n",
      "Step 30, Loss: 2.7853\n",
      "Step 31, Loss: 2.7415\n",
      "Step 32, Loss: 2.6956\n",
      "Step 33, Loss: 2.6426\n",
      "Step 34, Loss: 2.6001\n",
      "Step 35, Loss: 2.5486\n",
      "Step 36, Loss: 2.4946\n",
      "Step 37, Loss: 2.4490\n",
      "Step 38, Loss: 2.4068\n",
      "Step 39, Loss: 2.3586\n",
      "Step 40, Loss: 2.3082\n",
      "Step 41, Loss: 2.2733\n",
      "Step 42, Loss: 2.2182\n",
      "Step 43, Loss: 2.1777\n",
      "Step 44, Loss: 2.1352\n",
      "Step 45, Loss: 2.0970\n",
      "Step 46, Loss: 2.0572\n",
      "Step 47, Loss: 2.0151\n",
      "Step 48, Loss: 2.0062\n",
      "Step 49, Loss: 1.9424\n",
      "Step 50, Loss: 1.8913\n",
      "Step 51, Loss: 1.8472\n",
      "Step 52, Loss: 1.8207\n",
      "Step 53, Loss: 1.7696\n",
      "Step 54, Loss: 1.7369\n",
      "Step 55, Loss: 1.7028\n",
      "Step 56, Loss: 1.6536\n",
      "Step 57, Loss: 1.6263\n",
      "Step 58, Loss: 1.5849\n",
      "Step 59, Loss: 1.5502\n",
      "Step 60, Loss: 1.5142\n",
      "Step 61, Loss: 1.4819\n",
      "Step 62, Loss: 1.4432\n",
      "Step 63, Loss: 1.4127\n",
      "Step 64, Loss: 1.3768\n",
      "Step 65, Loss: 1.3463\n",
      "Step 66, Loss: 1.3142\n",
      "Step 67, Loss: 1.2835\n",
      "Step 68, Loss: 1.2534\n",
      "Step 69, Loss: 1.2199\n",
      "Step 70, Loss: 1.1937\n",
      "Step 71, Loss: 1.1631\n",
      "Step 72, Loss: 1.1357\n",
      "Step 73, Loss: 1.1075\n",
      "Step 74, Loss: 1.0805\n",
      "Step 75, Loss: 1.0545\n",
      "Step 76, Loss: 1.0270\n",
      "Step 77, Loss: 1.0012\n",
      "Step 78, Loss: 0.9766\n",
      "Step 79, Loss: 0.9517\n",
      "Step 80, Loss: 0.9277\n",
      "Step 81, Loss: 0.9051\n",
      "Step 82, Loss: 0.8829\n",
      "Step 83, Loss: 0.8632\n",
      "Step 84, Loss: 0.8401\n",
      "Step 85, Loss: 0.8156\n",
      "Step 86, Loss: 0.7964\n",
      "Step 87, Loss: 0.7725\n",
      "Step 88, Loss: 0.7565\n",
      "Step 89, Loss: 0.7357\n",
      "Step 90, Loss: 0.7176\n",
      "Step 91, Loss: 0.6972\n",
      "Step 92, Loss: 0.6799\n",
      "Step 93, Loss: 0.6606\n",
      "Step 94, Loss: 0.6445\n",
      "Step 95, Loss: 0.6286\n",
      "Step 96, Loss: 0.6160\n",
      "Step 97, Loss: 0.6128\n",
      "Step 98, Loss: 0.6249\n",
      "Step 99, Loss: 0.6414\n",
      "Step 100, Loss: 0.5931\n",
      "Done. Model runs successfully with nested optimization logic.\n"
     ]
    }
   ],
   "source": [
    "def toy_training_loop():\n",
    "    # Config\n",
    "    vocab_size = 100\n",
    "    dim = 64\n",
    "    depth = 2\n",
    "    head_dim = 16\n",
    "    seq_len = 32\n",
    "    batch_size = 4\n",
    "    \n",
    "    # Init Model\n",
    "    model = HOPEModel(vocab_size, dim, depth, head_dim)\n",
    "    \n",
    "    # HOPE introduces internal optimization loops. \n",
    "    # The outer optimizer (Meta-learner) trains the initialization \n",
    "    # and the learning rates of the inner modules.\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    print(\"Running HOPE Toy Task...\")\n",
    "    \n",
    "    # Dummy Data: Copy task (Input: A B C ... -> Target: A B C ...)\n",
    "    inputs = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    targets = torch.roll(inputs, shifts=-1, dims=1) # Next token prediction\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass runs the internal Nested Learning (inner optimizers)\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        # Loss calculation (Standard CE)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Step {step+1}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    print(\"Done. Model runs successfully with nested optimization logic.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    toy_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c00ac7",
   "metadata": {},
   "source": [
    "Loss after 100 iters. 4.7313-->0.5931"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
