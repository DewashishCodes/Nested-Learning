{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe78dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import Iterable, Optional, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a423a",
   "metadata": {},
   "source": [
    "Memory Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862cc327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDMemory:\n",
    "    def __init__(self, lr: float = 1e-2):\n",
    "        self.lr = float(lr)\n",
    "\n",
    "    def step(self, param: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
    "        return param - self.lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc6d73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MomentumMemory:\n",
    "    \"\"\"Momentum viewed as an associative memory (per-parameter states).\"\"\"\n",
    "    def __init__(self, lr: float = 1e-2, beta: float = 0.9, device=None):\n",
    "        self.lr = float(lr)\n",
    "        self.beta = float(beta)\n",
    "        self.state: Dict[int, torch.Tensor] = {}\n",
    "        self.device = device\n",
    "\n",
    "    def step(self, param: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
    "        pid = id(param)\n",
    "        if pid not in self.state:\n",
    "            self.state[pid] = torch.zeros_like(grad, device=grad.device)\n",
    "        m = self.state[pid]\n",
    "        m = self.beta * m + (1.0 - self.beta) * grad\n",
    "        self.state[pid] = m\n",
    "        return param - self.lr * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa892bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamMemory:\n",
    "    \"\"\"Adam reinterpreted as multi-level memory (m, v) per param.\"\"\"\n",
    "    def __init__(self, lr: float = 1e-3, b1: float = 0.9, b2: float = 0.999, eps: float = 1e-8):\n",
    "        self.lr = float(lr)\n",
    "        self.b1 = float(b1)\n",
    "        self.b2 = float(b2)\n",
    "        self.eps = float(eps)\n",
    "        self.state: Dict[int, Dict[str, torch.Tensor]] = {}\n",
    "        self.t: Dict[int, int] = {}\n",
    "\n",
    "    def step(self, param: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
    "        pid = id(param)\n",
    "        if pid not in self.state:\n",
    "            self.state[pid] = {\n",
    "                'm': torch.zeros_like(grad, device=grad.device),\n",
    "                'v': torch.zeros_like(grad, device=grad.device)\n",
    "            }\n",
    "            self.t[pid] = 0\n",
    "        s = self.state[pid]\n",
    "        self.t[pid] += 1\n",
    "        s['m'] = self.b1 * s['m'] + (1 - self.b1) * grad\n",
    "        s['v'] = self.b2 * s['v'] + (1 - self.b2) * (grad * grad)\n",
    "        t = self.t[pid]\n",
    "        m_hat = s['m'] / (1 - self.b1 ** t)\n",
    "        v_hat = s['v'] / (1 - self.b2 ** t)\n",
    "        update = self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "        return param - update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32047d87",
   "metadata": {},
   "source": [
    "Two NL-inspired extensions from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2afbb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMomentumMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Momentum: the momentum memory is an MLP that maps the incoming gradient\n",
    "    (or gradient features) to a momentum-like update. This adds representational power.\n",
    "    This class is a Module so it can be trained jointly if you want (requires hooking into\n",
    "    a higher-level training loop).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden=64, lr=1e-2, beta=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = float(lr)\n",
    "        self.beta = float(beta)\n",
    "        # small MLP to \"compress\" gradients into a learned momentum signal\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, dim)\n",
    "        )\n",
    "        # per-parameter running memory will be stored in python dict keyed by id(param)\n",
    "        self.state: Dict[int, torch.Tensor] = {}\n",
    "\n",
    "    def step(self, param: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        grad shape must be (N,) flattened or param-shaped; for simplicity we flatten param & grad,\n",
    "        pass through mlp (so mlp dim must match flat size) OR you can apply mlp per-element (1D conv).\n",
    "        Here we'll operate on param.view(-1) for demonstration (small params only).\n",
    "        \"\"\"\n",
    "        pid = id(param)\n",
    "        flat_grad = grad.view(-1)\n",
    "        # if mlp input/output dims don't match, we fallback to elementwise transform\n",
    "        if flat_grad.shape[0] != self.mlp[0].in_features:\n",
    "            # safe fallback: elementwise 1-layer learned scaler (vector of same shape)\n",
    "            # create or fetch per-param linear scaling vector stored as part of state (not ideal but simple)\n",
    "            if pid not in self.state:\n",
    "                self.state[pid] = torch.zeros_like(flat_grad, device=flat_grad.device)\n",
    "            # map gradient to momentum via a simple nonlinear transform (elementwise tanh)\n",
    "            learned = torch.tanh(flat_grad)  # placeholder compress\n",
    "            m = self.beta * self.state[pid] + (1.0 - self.beta) * learned\n",
    "            self.state[pid] = m\n",
    "            new_flat = flat_grad - self.lr * m\n",
    "            return new_flat.view_as(param)\n",
    "        else:\n",
    "            # MLP path: produce learned momentum vector\n",
    "            g_in = flat_grad.unsqueeze(0)  # [1, N]\n",
    "            m_pred = self.mlp(g_in).squeeze(0)  # [N]\n",
    "            if pid not in self.state:\n",
    "                self.state[pid] = torch.zeros_like(m_pred, device=m_pred.device)\n",
    "            m = self.beta * self.state[pid] + (1.0 - self.beta) * m_pred\n",
    "            self.state[pid] = m\n",
    "            new_flat = flat_grad - self.lr * m\n",
    "            return new_flat.view_as(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd830437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreconditionedMomentumMemory:\n",
    "    \"\"\"\n",
    "    Momentum memory that applies a learnable or provided preconditioner P to gradients before storing.\n",
    "    For simplicity we show diagonal preconditioner (vector), but could be matrix or low-rank.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=1e-2, beta=0.9, preconditioner: Optional[Iterable[float]]=None):\n",
    "        self.lr = float(lr)\n",
    "        self.beta = float(beta)\n",
    "        self.state = {}\n",
    "        # preconditioner can be a scalar or per-parameter function; here we store a callable or scalar\n",
    "        self.pre = preconditioner\n",
    "\n",
    "    def _apply_pre(self, grad: torch.Tensor) -> torch.Tensor:\n",
    "        if self.pre is None:\n",
    "            return grad\n",
    "        # If pre is callable, call with grad; if scalar, multiply\n",
    "        if callable(self.pre):\n",
    "            return self.pre(grad)\n",
    "        else:\n",
    "            return grad * float(self.pre)\n",
    "\n",
    "    def step(self, param: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
    "        grad_pre = self._apply_pre(grad)\n",
    "        pid = id(param)\n",
    "        if pid not in self.state:\n",
    "            self.state[pid] = torch.zeros_like(grad_pre)\n",
    "        m = self.state[pid]\n",
    "        m = self.beta * m + (1.0 - self.beta) * grad_pre\n",
    "        self.state[pid] = m\n",
    "        return param - self.lr * m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
